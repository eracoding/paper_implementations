{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WdUW_BDeGHs",
        "outputId": "73e80b12-069f-4c9c-b393-8df84dff1f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops>=0.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.8.0)\n",
            "Collecting vector-quantize-pytorch>=1.6.28 (from -r requirements.txt (line 2))\n",
            "  Downloading vector_quantize_pytorch-1.18.5-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.5.0+cu121)\n",
            "Collecting einx>=0.3.0 (from vector-quantize-pytorch>=1.6.28->-r requirements.txt (line 2))\n",
            "  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->-r requirements.txt (line 3)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->-r requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->-r requirements.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->-r requirements.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->-r requirements.txt (line 3)) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->-r requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from einx>=0.3.0->vector-quantize-pytorch>=1.6.28->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.10/dist-packages (from einx>=0.3.0->vector-quantize-pytorch>=1.6.28->-r requirements.txt (line 2)) (2.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->-r requirements.txt (line 3)) (3.0.2)\n",
            "Downloading vector_quantize_pytorch-1.18.5-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einx-0.3.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einx, vector-quantize-pytorch\n",
            "Successfully installed einx-0.3.0 vector-quantize-pytorch-1.18.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4fLY6Lhd8Tk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from discrete_kv_bottleneck import DiscreteKeyValueBottleneck, DiscreteKeyValueBottleneckConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgnd7srTd8To"
      },
      "outputs": [],
      "source": [
        "config = DiscreteKeyValueBottleneckConfig(\n",
        "    dim=128, # input dimension\n",
        "    num_memories=256, # output dimension - or dimension of each memories for all heads (defaults to same as input)\n",
        "    num_memory_codebooks=4,\n",
        "    average_pool_memories=True\n",
        ")\n",
        "\n",
        "kvbottleneck = DiscreteKeyValueBottleneck(config)\n",
        "input_tensor = torch.randn(2, 64, 128)  # Example input\n",
        "memories = kvbottleneck(input_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhmJlawSexDb",
        "outputId": "12fcf25d-4928-4705-a445-42b24230e877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vit-pytorch\n",
            "  Downloading vit_pytorch-1.8.5-py3-none-any.whl.metadata (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: einops>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from vit-pytorch) (0.8.0)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from vit-pytorch) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from vit-pytorch) (0.20.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10->vit-pytorch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->vit-pytorch) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->vit-pytorch) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->vit-pytorch) (3.0.2)\n",
            "Downloading vit_pytorch-1.8.5-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vit-pytorch\n",
            "Successfully installed vit-pytorch-1.8.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUgo2J0sd8Tp",
        "outputId": "4975bb67-0cb9-49f3-b72e-e0087f085c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 2048])\n"
          ]
        }
      ],
      "source": [
        "from vit_pytorch import SimpleViT\n",
        "from vit_pytorch.extractor import Extractor\n",
        "import torch\n",
        "from discrete_kv_bottleneck import DiscreteKeyValueBottleneck, DiscreteKeyValueBottleneckConfig\n",
        "\n",
        "# Initialize SimpleViT\n",
        "vit = SimpleViT(\n",
        "    image_size=256,\n",
        "    patch_size=32,\n",
        "    num_classes=1000,\n",
        "    dim=512,\n",
        "    depth=6,\n",
        "    heads=16,\n",
        "    mlp_dim=2048\n",
        ")\n",
        "\n",
        "# Train vit, or load pretrained weights\n",
        "# Assuming vit is pretrained, extract only embeddings\n",
        "vit = Extractor(vit, return_embeddings_only=True)\n",
        "\n",
        "# Configure the DiscreteKeyValueBottleneck\n",
        "config = DiscreteKeyValueBottleneckConfig(\n",
        "    encoder=vit,         # pass the frozen encoder into the bottleneck\n",
        "    dim=512,             # input dimension\n",
        "    num_memories=256,    # number of memories\n",
        "    dim_memory=2048,     # dimension of the output memories\n",
        "    average_pool_memories=True\n",
        ")\n",
        "\n",
        "# Initialize the bottleneck module\n",
        "enc_with_bottleneck = DiscreteKeyValueBottleneck(config, decay=0.9)\n",
        "\n",
        "# Example input images\n",
        "images = torch.randn(1, 3, 256, 256)  # input to encoder\n",
        "\n",
        "# Process the images through the encoder with bottleneck\n",
        "memories = enc_with_bottleneck(images)  # Output: (1, 64, 2048)\n",
        "\n",
        "print(memories.shape)  # Should print: torch.Size([1, 64, 2048])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the output:\n",
        "- Input Images: we have passed an input tensor of shape (1, 3, 256, 256), representing a batch of one image with three color channels (RGB) and a size of 256x256 pixels.\n",
        "- SimpleViT Embeddings: The SimpleViT processes the image into patches and returns embeddings of size 512. By using the Extractor, the ViT acts as a frozen encoder, providing a feature map of size (1, 64, 512), where 64 is the number of patches, and 512 is the embedding dimension for each patch.\n",
        "- Discrete Key-Value Bottleneck Output: The embeddings from ViT are then fed into the bottleneck, which processes them and produces output memories with shape (1, 64, 2048). Here, 64 corresponds to the number of input patches, and 2048 represents the processed, compressed memory dimension.\n",
        "\n",
        "- Batch Size: 1 — One input image processed.\n",
        "- Number of Patches: 64 — The image was split into 64 patches by the Vision Transformer (ViT), based on the patch size.\n",
        "- Memory Dimension: 2048 — Each patch is processed into a memory vector of dimension 2048 by the Discrete Key-Value Bottleneck.\n",
        "\n",
        "### Interpretation: Analysis of Results\n",
        "Objective:\n",
        "The aim was to integrate a discrete key-value bottleneck mechanism with a Vision Transformer (ViT) model, leveraging the bottleneck to transform high-dimensional inputs into compressed memory representations. This can help achieve efficient information processing and storage.\n",
        "\n",
        "Key Observations:\n",
        "Effective Feature Compression:\n",
        "\n",
        "The input image is processed by SimpleViT, which splits it into 64 patches, and each patch is represented as a 512-dimensional embedding.\n",
        "After passing through the Discrete Key-Value Bottleneck, the embeddings are transformed into discrete memory representations of size 2048.\n",
        "This increase in dimension might initially seem counterintuitive, but it allows the bottleneck to encode more complex information in a structured way, potentially enabling better feature representation and retrieval.\n",
        "Average Pooling:\n",
        "\n",
        "Average pooling is applied across multiple discrete memories, which helps aggregate information and reduce noise. This makes the final output more robust and reduces variability across different patches.\n",
        "Memory and Efficiency:\n",
        "\n",
        "The bottleneck mechanism compresses and quantizes input features, which can be beneficial for downstream tasks, such as classification or image generation, where discrete representations help with better generalization.\n",
        "\n",
        "Potential Applications:\n",
        "- Efficient Storage and Retrieval: By transforming continuous features into discrete memory tokens, this setup can be used in scenarios that require efficient storage and retrieval of information, such as language models or image generation tasks.\n",
        "- Robust Representations: The process can also aid in creating more robust representations that are less prone to noise, improving model performance in tasks like classification."
      ],
      "metadata": {
        "id": "PhrHpOaAfKG2"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}