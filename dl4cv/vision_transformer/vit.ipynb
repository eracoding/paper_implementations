{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "# from torchvision import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    #def __init__(self, dim_in: int, dim_q: int, dim_k: int): # mistake, from the paper it says dim_q = dim_k but not dim_k != dim_v\n",
    "    def __init__(self, dim_in: int, dim_qk: int, dim_v: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_qk)\n",
    "        self.k = nn.Linear(dim_in, dim_qk)\n",
    "        self.v = nn.Linear(dim_in, dim_v)\n",
    "    \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        q = self.q(query)\n",
    "        k = self.k(key)\n",
    "        v = self.v(value)\n",
    "\n",
    "        return self.scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computed scaled dot product of attention.\n",
    "\n",
    "        Args:\n",
    "            q (torch.Tensor): Query tensor of shape (batch_size, seq_len, dim_qk).\n",
    "            k (torch.Tensor): Key tensor of shape (batch_size, seq_len, dim_qk).\n",
    "            v (torch.Tensor): Value tensor of shape (batch_size, seq_len, dim_v).\n",
    "            mask (torch.Tensor, optional): Mask tensor of shape (batch_size, seq_len, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after attention is applied.\n",
    "        \"\"\"\n",
    "        # Scaled dot-product\n",
    "        scores = q.bmm(k.transpose(1, 2)) / torch.sqrt(torch.tensor(q.size(-1), dtype=torch.float32) + 1e-8)\n",
    "\n",
    "        # Apply mask if needed\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Softmax across the last dim\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Compute the output\n",
    "        output = attention_weights.bmm(v)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_qk: int, dim_v: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_qk, dim_v) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(num_heads * dim_qk, dim_in)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None):\n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value, mask) for h in self.heads], dim=-1)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.py\n",
    "\n",
    "def position_encoding(seq_len: int, dim_model: int, device: torch.device = torch.device('cpu')) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generates sinusoidal positional encodings.\n",
    "\n",
    "    Args:\n",
    "        seq_len (int): The length of the sequence.\n",
    "        dim_model (int): The dimensionality of the model (must be even for sin-cos pairing).\n",
    "        device (torch.device): The device on which to create the encoding tensor.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: The positional encoding tensor of shape (1, seq_len, dim_model)\n",
    "    \"\"\"\n",
    "    # Position indices for each position in the sequence (shape: [1, seq_len, 1])\n",
    "    pos = torch.arange(seq_len, dtype=torch.float32, device=device).reshape(1, -1, 1)\n",
    "\n",
    "    # Dimension indices for each encoding dimension (shape: [1, 1, dim_model])\n",
    "    dim = torch.arange(dim_model, dtype=torch.float32, device=device).reshape(1, 1, -1)\n",
    "\n",
    "    # Compute the phase values based on position and dimension indices\n",
    "    # This uses `dim_model` to scale each dimension in the sinusoidal pattern\n",
    "    phase = pos / 1e4 ** (dim / dim_model)\n",
    "\n",
    "    # Apply sin to even dimensions and cos to odd dimensions\n",
    "    encoding = torch.zeros((1, seq_len, dim_model), device=device)\n",
    "    encoding[..., 0::2] = torch.sin(phase[..., 0::2]) # Apply sin to even dimensions\n",
    "    encoding[..., 1::2] = torch.cos(phase[..., 1::2]) # Apply cos to odd dimensions\n",
    "\n",
    "    return encoding\n",
    "    \n",
    "\n",
    "def feed_forward(dim_model: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Fully connected feed-forward network, which is applied to each position separately and identically.\n",
    "\n",
    "    Args:\n",
    "        dim_model (int): The dimensionality of the model.\n",
    "        dim_feedforward (int): inner-layer dimensionality.\n",
    "    \n",
    "    Returns:\n",
    "        nn.Module: Sequential layer of feed-forward neural network.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_model, dim_feedforward),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dim_feedforward, dim_model),\n",
    "    ) # Possible improvements from vanila ViT is adding dropout, layer norm, and etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnections(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Residual connection with layer normalization and optional dropout.\n",
    "        \n",
    "        Args:\n",
    "            sublayer (nn.Module): The sublayer to be applied, e.g., MultiHeadAttention or FeedForward.\n",
    "            dimension (int): The dimension for the LayerNorm.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, *tensors: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the residual layer.\n",
    "        \n",
    "        Assumes that the first tensor in `tensors` is the primary input for the residual connection.\n",
    "        \n",
    "        Args:\n",
    "            *tensors: Input tensors where the first is assumed to be the main residual input.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The output tensor after applying residual connection, dropout, and normalization.\n",
    "        \"\"\"\n",
    "        # Apply sublayer, then add the residual connection, followed by normalization\n",
    "        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Transformer Encoder Layer with residual connections around multi-head\n",
    "        attention and feed-forward sub-layers.\n",
    "\n",
    "        Args:\n",
    "            dim_model (int): Dimension of the model embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dim_feedforward (int): Dimension of the feed-forward layer.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Calculate dimensions for query/key and value based on number of heads\n",
    "        dim_qk = dim_v = max(dim_model // num_heads, 1)\n",
    "        \n",
    "        # Multi-head attention layer with residual connection \n",
    "        self.attention = ResidualConnections(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_qk, dim_v),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Feed-forward network with residual connection\n",
    "        self.feed_forward = ResidualConnections(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Forward pass through the Transformer Encoder Layer.\n",
    "        \n",
    "        Args:\n",
    "            src (Tensor): Input tensor of shape (batch_size, seq_len, dim_model).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of the same shape as input.\n",
    "        \"\"\"\n",
    "        src = self.attention(src, src, src)\n",
    "        return self.feed_forward(src)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        apply_positional_encoding: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Transformer Encoder comprising multiple encoder layers.\n",
    "\n",
    "        Args:\n",
    "            num_layers (int): Number of encoder layers.\n",
    "            dim_model (int): Dimension of the model embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dim_feedforward (int): Dimension of the feed-forward layer.\n",
    "            dropout (float): Dropout probability.\n",
    "            apply_positional_encoding (bool): Whether to add positional encoding.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.apply_positional_encoding = apply_positional_encoding\n",
    "\n",
    "        # Stack of encoder layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def add_positional_encoding(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Adds positional encoding to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Input tensor of shape (batch_size, seq_len, dim_model).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Input tensor with positional encoding added.\n",
    "        \"\"\"\n",
    "        seq_len, dimension = src.size(1), src.size(2)\n",
    "        return src + position_encoding(seq_len, dimension, device=src.device)\n",
    "\n",
    "    def forward(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the Transformer Encoder.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Input tensor of shape (batch_size, seq_len, dim_model).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Encoded tensor of the same shape as input.\n",
    "        \"\"\"\n",
    "        # Optionally add positional encoding\n",
    "        if self.apply_positional_encoding:\n",
    "            src = self.add_positional_encoding(src)\n",
    "\n",
    "        # Pass through each encoder layer\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "        \n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder module is quite similar to the encoder, with just a few small differences:\n",
    "- The decoder accepts two inputs (the target sequence and the encoder memory), rather than one input.\n",
    "- There are two multi-head attention modules per layer (the target sequence self-attention module and the decoder-encoder attention module) rather than just one.\n",
    "- The second multi-head attention module, rather than strict self attention, expects the encoder memory as $K$ and $V$.\n",
    "- Since accessing future elements of the target sequence would be \"cheating,\" we need to mask out future elements of the input target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Transformer Decoder Layer with two residual-attention connections and a \n",
    "        feed-forward network, each wrapped in residual connections.\n",
    "\n",
    "        Args:\n",
    "            dim_model (int): Dimension of the model embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dim_feedforward (int): Dimension of the feed-forward layer.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Dimensions for query/key and value\n",
    "        dim_qk = dim_v = max(dim_model // num_heads, 1)\n",
    "\n",
    "        # Self-attention with residual connection\n",
    "        self.attention1 = ResidualConnections(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_qk, dim_v),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Cross-attention with residual connection\n",
    "        self.attention2 = ResidualConnections(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_qk, dim_v),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Feed-forward with residual connection\n",
    "        self.feed_forward = ResidualConnections(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout\n",
    "        )\n",
    "    \n",
    "    def forward(self, trg: torch.Tensor, memory: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the Transformer Decoder Layer.\n",
    "        \n",
    "        Args:\n",
    "            trg (Tensor): Target tensor of shape (batch_size, seq_len, dim_model).\n",
    "            memory (Tensor): Memory tensor from the encoder of shape (batch_size, seq_len, dim_model).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, seq_len, dim_model).\n",
    "        \"\"\"\n",
    "        trg = self.attention1(trg, trg, trg) # Self-attention\n",
    "        trg = self.attention2(trg, memory, memory) # Cross-attention with encoder output\n",
    "        return self.feed_forward(trg)\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        apply_positional_encoding: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Transformer Decoder consisting of multiple decoder layers.\n",
    "\n",
    "        Args:\n",
    "            num_layers (int): Number of decoder layers.\n",
    "            dim_model (int): Dimension of the model embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dim_feedforward (int): Dimension of the feed-forward layer.\n",
    "            dropout (float): Dropout probability.\n",
    "            apply_positional_encoding (bool): Whether to add positional encoding.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.apply_positional_encoding = apply_positional_encoding\n",
    "\n",
    "        # Stack of decoder layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Final linear layer to project to output vocabulary size or target dimension\n",
    "        self.linear = nn.Linear(dim_model, dim_model)\n",
    "    \n",
    "    def add_positional_encoding(self, trg: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Adds positional encoding to the target tensor.\n",
    "        \n",
    "        Args:\n",
    "            trg (Tensor): Target tensor of shape (batch_size, seq_len, dim_model).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Target tensor with positional encoding added.\n",
    "        \"\"\"\n",
    "        seq_len, dimension = trg.size(1), trg.size(2)\n",
    "        return trg + position_encoding(seq_len, dimension, device=trg.device)\n",
    "\n",
    "    def forward(self, trg: torch.Tensor, memory: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the Transformer Decoder.\n",
    "        \n",
    "        Args:\n",
    "            trg (Tensor): Target tensor of shape (batch_size, seq_len, dim_model).\n",
    "            memory (Tensor): Memory tensor from the encoder of shape (batch_size, seq_len, dim_model).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Decoded tensor of shape (batch_size, seq_len, dim_model).\n",
    "        \"\"\"\n",
    "        # Optionally add positional encoding\n",
    "        if self.apply_positional_encoding:\n",
    "            trg = self.add_positional_encoding(trg)\n",
    "\n",
    "        # Pass through each decoder layer\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, memory)\n",
    "        \n",
    "        # Linear projection, softmax should be applied externally if needed\n",
    "        # return torch.softmax(self.linear(trg), dim=-1) #The softmax should typically be applied outside the decoder in a final output layer if necessary.\n",
    "        return self.linear(trg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Transformer model consisting of an encoder and decoder.\n",
    "\n",
    "        Args:\n",
    "            num_encoder_layers (int): Number of layers in the encoder.\n",
    "            num_decoder_layers (int): Number of layers in the decoder.\n",
    "            dim_model (int): Dimension of the model.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dim_feedforward (int): Dimension of the feed-forward network.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_encoder_layers, \n",
    "            dim_model=dim_model, \n",
    "            num_heads=num_heads, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.decoder = TransformerDecoder(\n",
    "            num_layers=num_decoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, src: torch.Tensor, trg: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source tensor of shape (batch_size, src_seq_len, dim_model).\n",
    "            trg (Tensor): Target tensor of shape (batch_size, trg_seq_len, dim_model).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, trg_seq_len, dim_model).\n",
    "        \"\"\"\n",
    "        # Pass through encoder\n",
    "        memory = self.encoder(src)\n",
    "\n",
    "        # Pass through decoder with the encoder's output as memory\n",
    "        output = self.decoder(trg, memory)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "src = torch.rand(64, 32, 512)\n",
    "tgt = torch.rand(64, 16, 512)\n",
    "out = Transformer()(src, tgt)\n",
    "print(out.shape)\n",
    "# torch.Size([64, 16, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "\n",
    "The steps of ViT are as follows:\n",
    "\n",
    "1. Split input image into patches\n",
    "2. Flatten the patches\n",
    "3. Produce linear embeddings from the flattened patches\n",
    "4. Add position embeddings\n",
    "5. Feed the sequence preceeded by a `[class]` token as input to a standard transformer encoder\n",
    "6. Pretrain the model to ouptut image labels for the `[class]` token (fully supervised on a huge dataset such as ImageNet-22K)\n",
    "7. Fine-tune on the downstream dataset for the specific image classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention (MSA) layer for Transformer-based models.\n",
    "\n",
    "    This layer splits the input into multiple attention heads, computes self-attention for each head, \n",
    "    and concatenates the results. It is commonly used in Transformer architectures for both NLP and \n",
    "    Vision Transformers (ViT).\n",
    "\n",
    "    Args:\n",
    "        dim_model (int): The dimensionality of the input and output representations.\n",
    "        num_heads (int): The number of attention heads. Each head will have a dimensionality of `dim_model / num_heads`.\n",
    "\n",
    "    Attributes:\n",
    "        q_linear (nn.Linear): Linear layer to project input to query vectors.\n",
    "        k_linear (nn.Linear): Linear layer to project input to key vectors.\n",
    "        v_linear (nn.Linear): Linear layer to project input to value vectors.\n",
    "        out_linear (nn.Linear): Final linear layer to project concatenated head outputs back to `dim_model`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int,\n",
    "        num_heads: int = 2,\n",
    "    ):  \n",
    "        \n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "\n",
    "        # Ensure the model dimension is divisible by the number of heads\n",
    "        assert dim_model % num_heads == 0, f\"dim_model {dim_model} must be divisible by num_heads {num_heads}\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_head = dim_model // num_heads\n",
    "\n",
    "        # Linear layers to project input into query, key, and value spaces\n",
    "        self.q = nn.Linear(dim_model, dim_model)\n",
    "        self.k = nn.Linear(dim_model, dim_model)\n",
    "        self.v = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "        # Output projection layer\n",
    "        self.out_linear = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head self-attention.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, dim_model), where:\n",
    "                - batch_size is the number of input samples,\n",
    "                - seq_len is the sequence length, and\n",
    "                - dim_model is the dimensionality of each token in the sequence.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, seq_len, dim_model) after multi-head self-attention is applied.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, dim_model = x.size()\n",
    "\n",
    "        # Project inputs to multi-head query, key, and value spaces\n",
    "        q = self.q(x).view(batch_size, seq_len, self.num_heads, self.dim_head).transpose(1, 2)\n",
    "        k = self.k(x).view(batch_size, seq_len, self.num_heads, self.dim_head).transpose(1, 2)\n",
    "        v = self.v(x).view(batch_size, seq_len, self.num_heads, self.dim_head).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.dim_head ** 0.5)  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)  # Apply softmax to get attention weights\n",
    "        attended_values = torch.matmul(attention_weights, v)  # Shape: (batch_size, num_heads, seq_len, dim_head)\n",
    "        \n",
    "        # Concatenate heads and project to output dimension\n",
    "        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, seq_len, dim_model)\n",
    "        output = self.out_linear(attended_values)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT) implementation.\n",
    "\n",
    "    This model divides an input image into patches, embeds them, adds positional encoding, \n",
    "    and applies Transformer layers to produce a classification output.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input image (channels, height, width).\n",
    "        n_patches (int): Number of patches along each dimension.\n",
    "        hidden_d (int): Dimensionality of the hidden layer.\n",
    "        n_heads (int): Number of attention heads in multi-head self-attention.\n",
    "        out_d (int): Dimensionality of the output (number of classes for classification).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,\n",
    "        n_patches=7,\n",
    "        hidden_d=8,\n",
    "        n_heads=2,\n",
    "        out_d=10,\n",
    "    ):\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # Input and patch size checks\n",
    "        self.input_shape = input_shape\n",
    "        self.n_patches = n_patches\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert input_shape[1] % n_patches == 0, \"Input height not divisible by number of patches\"\n",
    "        assert input_shape[2] % n_patches == 0, \"Input width not divisible by number of patches\"\n",
    "\n",
    "        self.patch_size = (input_shape[1] / n_patches, input_shape[2] / n_patches)\n",
    "        self.hidden_d = hidden_d\n",
    "\n",
    "        # 1) Linear layers for patch embeddings\n",
    "        self.input_d = int(input_shape[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d, hidden_d)\n",
    "\n",
    "        # 2) Classification Token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # 3) Positional embedding - forward layer\n",
    "\n",
    "        # 4a) Layer Normalization 1\n",
    "        self.ln1 = nn.LayerNorm((self.n_patches * self.n_patches + 1, self.hidden_d))\n",
    "\n",
    "        # 4b) MSA and classification token\n",
    "        self.msa = MultiHeadSelfAttention(self.hidden_d, n_heads)\n",
    "\n",
    "        # 5a) Layer Normalization 2\n",
    "        self.ln2 = nn.LayerNorm((self.n_patches * self.n_patches + 1, self.hidden_d))\n",
    "\n",
    "        # 5b) Encoder MLP\n",
    "        self.enc_mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, self.hidden_d),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 6) Classification MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, out_d),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Vision Transformer.\n",
    "\n",
    "        Args:\n",
    "            images (Tensor): Batch of images of shape (batch_size, channels, height, width).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output logits of shape (batch_size, out_d).\n",
    "        \"\"\"\n",
    "        device = images.device\n",
    "\n",
    "        # Divide images into patches and flatten each patch\n",
    "        n, c, w, h = images.shape\n",
    "        patches = images.reshape(n, self.n_patches ** 2, self.input_d)\n",
    "\n",
    "        # Running linear layer for tokenization\n",
    "        tokens = self.linear_mapper(patches).to(device)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        class_token = self.class_token.to(device)\n",
    "        tokens = torch.stack([torch.vstack((class_token, tokens[i])) for i in range(len(tokens))])\n",
    "\n",
    "        # Adding positional embedding\n",
    "        tokens += position_encoding(self.n_patches ** 2 + 1, self.hidden_d, device).repeat(n, 1, 1)\n",
    "\n",
    "        # TRANSFORMER ENCODER BEGINS ###################################\n",
    "        # NOTICE: MULTIPLE ENCODER BLOCKS CAN BE STACKED TOGETHER ######\n",
    "        # Running Layer Normalization, MSA and residual connection\n",
    "        tokens = tokens.to(device)\n",
    "        ln1_tokens = self.ln1(tokens)\n",
    "        msa_out = self.msa(ln1_tokens)\n",
    "        out = tokens + msa_out\n",
    "\n",
    "        # Running Layer Normalization, MLP and residual connection\n",
    "        out = out + self.enc_mlp(self.ln2(out))\n",
    "        # TRANSFORMER ENCODER ENDS   ###################################\n",
    "\n",
    "        # Getting the classification token only\n",
    "        out = out[:, 0]\n",
    "\n",
    "        return self.mlp(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vit(model, device, train_loader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target) / len(data)\n",
    "\n",
    "            train_loss += loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} loss: {train_loss:.2f}\")\n",
    "\n",
    "\n",
    "def test_vit(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss\n",
    "\n",
    "        correct += torch.sum(torch.argmax(output, dim=1) == target).item()\n",
    "        total += len(data)\n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1133)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:03<00:00, 2779455.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1133)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 102937.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1133)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 899341.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1133)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 1135785.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_set = datasets.MNIST(root='./datasets', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root='./datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ViT((1, 28, 28), n_patches=7, hidden_d=20, n_heads=2, out_d=10)\n",
    "model = model.to(device)\n",
    "\n",
    "N_EPOCHS = 1\n",
    "LR = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 loss: 526.00\n"
     ]
    }
   ],
   "source": [
    "train_vit(model, device, train_loader, optimizer, criterion, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1435.14\n",
      "Test accuracy: 16.49%\n"
     ]
    }
   ],
   "source": [
    "test_vit(model, device, test_loader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
